# nb-transcribe üéôÔ∏è

[![Build and Push Docker Images](https://github.com/jenanos/nb-transcribe/actions/workflows/build-and-push.yml/badge.svg)](https://github.com/jenanos/nb-transcribe/actions/workflows/build-and-push.yml)

End-to-end Norwegian speech-to-text with a FastAPI backend, Next.js 15 frontend, GPU-accelerated NB-Whisper transcription, and Gemma-3 assisted copy editing.

## ‚ú® What‚Äôs inside

- **FastAPI backend** that exposes both synchronous `/process/` and async `/jobs` endpoints.
- **NB-Whisper Large** for GPU-accelerated automatic speech recognition.
- **Gemma-3 4B IT** for summarising, rewriting, and workflow extraction.
- **Stub mode** (`DEV_STUB=1`) to exercise the UI without a GPU, HF token, or FFmpeg.
- **Docker Compose** definitions for a full-stack deployment with NVIDIA GPU support.

## üß∞ Prerequisites

- Git and a POSIX-compatible shell (macOS, Linux, or WSL2).
- Node.js 20+ with npm (consider using `nvm`).
- Python 3.11+ with `venv`.
- FFmpeg in your `PATH` (e.g. `sudo apt install ffmpeg`).
- NVIDIA GPU with CUDA 12.x and drivers installed for the full pipeline.
- Hugging Face account with access to Gemma-3 and a personal access token for `HF_TOKEN`.

## üöÄ Local development

### 1. Clone the repository

```bash
git clone https://github.com/<your-org>/nb-transcribe.git
cd nb-transcribe
```

### 2. Start the backend (full pipeline)

```bash
cd backend
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
export HF_TOKEN="<your-hf-token>"
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

The backend expects a CUDA-capable GPU, FFmpeg, and a valid Hugging Face token to load the transcription and rewriting models.

### 3. Start the frontend

```bash
cd ../frontend
npm install
cp .env.local.example .env.local  # adjust values when needed
npm run dev
```

The Next.js app runs on `http://localhost:3000` and forwards API calls to `BACKEND_URL` (defaults to the backend started above).
Toggle the mocked experience by setting `NEXT_PUBLIC_MOCK_MODE`/`MOCK_MODE` to `1` in `.env.local`.

## üß™ Backend stub mode (no GPU required)

Set `DEV_STUB=1` to skip heavy model loading and return deterministic demo responses:

```bash
cd backend
source .venv/bin/activate  # reuse the virtual environment created above
export DEV_STUB=1
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

With the stub enabled you can iterate on the frontend without FFmpeg, CUDA, or `HF_TOKEN`.

## üß™ Frontend mock mode

- Copy `frontend/.env.local.example` to `.env.local` and keep `NEXT_PUBLIC_MOCK_MODE=1` / `MOCK_MODE=1` to build the demo UI without a backend.
- The file picker is pre-filled with a demo clip; uploading new audio is disabled and shows a short English explainer.
- Start the transcription straight away to see mocked raw and rewritten outputs for each mode.

## üß∑ Useful commands

| Area     | Command                          | Notes |
|----------|----------------------------------|-------|
| Backend  | `pytest`                         | Runs API and pipeline tests (requires `DEV_STUB=1` for fast execution). |
| Backend  | `uvicorn main:app --reload`      | Starts the FastAPI server locally. |
| Frontend | `npm run lint`                   | Next.js linting. |
| Frontend | `npm test`                       | Jest + Testing Library suite. |

## üê≥ Docker Compose

Build and run the complete stack with GPU support (requires the NVIDIA Container Toolkit):

```bash
export HF_TOKEN="<your-hf-token>"
docker compose up --build
```

- The backend container reads `HF_TOKEN` at startup to authenticate against Hugging Face.
- The frontend is served on port 3000; the backend listens on port 8000 within the internal network.

## ‚öôÔ∏è Configuration

- `frontend/.env.local.example` ‚Äì template for local/frontend deployments (mock mode flags and backend URL).
- `backend/env.example` ‚Äì template for backend deployments (stub toggle and Hugging Face token).
- `HF_TOKEN` ‚Äì required by the backend for Gemma-3 powered rewriting when not in stub mode.
- `DEV_STUB` ‚Äì enable to run the backend with fixture data and without GPU dependencies.
- `BACKEND_URL` ‚Äì frontend override for API base URL (defaults to `http://127.0.0.1:8000`).

## üìÇ Repository layout

```
.
‚îú‚îÄ‚îÄ backend/        # FastAPI app with transcription and rewriting pipelines
‚îú‚îÄ‚îÄ frontend/       # Next.js 15 (App Router) UI
‚îî‚îÄ‚îÄ docker-compose.yml
```

## ü§ù Contributing

- Keep an eye on GPU VRAM usage: NB-Whisper Large and Gemma-3 both run on the GPU.
- Open issues or pull requests with ideas, bug fixes, or documentation improvements‚Äîcontributions are welcome!
